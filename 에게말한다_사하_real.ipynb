{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "에게말한다 사하_real",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOp1KKb0OfOPG7WvxUUSRgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ourlocalpetition/crawling/blob/master/%EC%97%90%EA%B2%8C%EB%A7%90%ED%95%9C%EB%8B%A4_%EC%82%AC%ED%95%98_real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbUvuoMJe8hv"
      },
      "source": [
        "#Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5m-hNmiRpi-"
      },
      "source": [
        "# drive mount\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "my_path = '/content/notebooks'\n",
        "os.symlink('/content/gdrive/Shared drives/우주청/데이터분석/colab_packages', my_path)\n",
        "sys.path.insert(0, my_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPL7IXS4_kaS"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "    \n",
        "drive.mount('/content/gdrive')\n",
        "sys.path.insert(0, '/content/gdrive/Shared drives/우주청/데이터분석/colab_packages')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOtJe5xfBs2"
      },
      "source": [
        "#set direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yH9mf7y_mPS"
      },
      "source": [
        "congress_dir='/content/gdrive/Shared drives/우주청/데이터분석/구청사이트분석'\n",
        "os.chdir(congress_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO4tx0lLAgjp"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZoqyUZfFRk"
      },
      "source": [
        "# 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrMvE7nxAjQZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anu7uZTHAsZA"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import pickle\n",
        "import csv\n",
        "import swifter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "from gensim.models.ldamodel import LdaModel \n",
        "from gensim.models.callbacks import CoherenceMetric \n",
        "from gensim import corpora \n",
        "from gensim.models.callbacks import PerplexityMetric \n",
        "import logging \n",
        "import pyLDAvis.gensim \n",
        "from gensim.models.coherencemodel import CoherenceModel \n",
        "\n",
        "dir_font='/content/gdrive/Shared drives/우주청/데이터분석/font/NanumSquareOTF_acL.otf'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1kA1C_1cw3"
      },
      "source": [
        "list_no=['민원','개선','사항','주민','관련','다대','주민','요청','첨부파일','아파트','우리','사하','사하구','저희','수고','안녕','대한','사람','구민','양천구','양천구청','서울시','답변','지역','신월동','목동','청장','생각','구청','위해','이행','생각','문제','대해']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90MOBDp2AuOd"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    한글,영문,숫자 남기고 제거한다.\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(r'[^ ㄱ-ㅣ가-힣A-Za-z]', '', text) #특수기호 제거, 정규 표현식    \n",
        "\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwUzEarxAvx6"
      },
      "source": [
        "def get_nouns(sentence):\n",
        "    \"\"\"\n",
        "    길이가 2이상인 단어중 명사(Noun)만을 반환한다.\n",
        "    불용어도 제거\n",
        "    \"\"\"\n",
        "    word=[]\n",
        "    tonkenizer=Okt()\n",
        "    tagged=tonkenizer.pos(sentence)\n",
        "    nouns=[s for s,t in tagged if t in['Noun'] and len(s)>1]\n",
        "    for i in nouns:\n",
        "        if i not in list_no:\n",
        "            word.append(i)\n",
        "    return word\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXmvEXeFAxIC"
      },
      "source": [
        "def tokenize(df):\n",
        "    df=clean_text(df.replace('\\n','').strip())\n",
        "    df=get_nouns(df)    \n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2VhQc9kyfqL"
      },
      "source": [
        "def del_sentence(list_del,df):\n",
        "    for d in list_del:\n",
        "        for num in range(len(df)-1,0,-1):\n",
        "            if d in df['token_title'][num]:\n",
        "                df=df.drop(num)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61vSo2sH4Ngg"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\" Compute c_v coherence for various number of topics \n",
        "    Parameters:\n",
        "    ---------- \n",
        "    dictionary : Gensim dictionary \n",
        "    corpus : Gensim corpus \n",
        "    texts : List of input texts \n",
        "    limit : Max num of topics \n",
        "\n",
        "    Returns: \n",
        "    ------- \n",
        "    model_list : List of LDA topic models \n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics \n",
        "    \"\"\" \n",
        "    coherence_values=[]\n",
        "    model_list = [] \n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        model_list.append(model) \n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v') \n",
        "        coherence_values.append(coherencemodel.get_coherence()) \n",
        "    return model_list, coherence_values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGZ0aawila5l"
      },
      "source": [
        "def clean_sentence(text):\n",
        "    \"\"\"\n",
        "    \\n삭제\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub('\\n', '', text) #특수기호 제거, 정규 표현식    \n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ReVtRJ2xHz"
      },
      "source": [
        "def find_optimal_number_of_topics(dictionary, corpus, processed_data): \n",
        "    limit = 40; \n",
        "    start = 2; \n",
        "    step = 6; \n",
        "    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step) \n",
        "    \n",
        "    x = range(start, limit, step) \n",
        "    plt.plot(x, coherence_values) \n",
        "    plt.xlabel(\"Num Topics\") \n",
        "    plt.ylabel(\"Coherence score\") \n",
        "    plt.legend((\"coherence_values\"), loc='best') \n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q8oHAGyiyiv"
      },
      "source": [
        "def make_topictable_per_doc(ldamodel, corpus):\n",
        "    topic_table = pd.DataFrame()\n",
        "\n",
        "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
        "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
        "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
        "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
        "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
        "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
        "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
        "\n",
        "        # 모든 문서에 대해서 각각 아래를 수행\n",
        "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
        "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
        "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
        "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
        "            else:\n",
        "                break\n",
        "    return(topic_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQSqGdsWcxL"
      },
      "source": [
        "def find_topic(topic,list_st):\n",
        "    result_list=[]\n",
        "    for i in topic:\n",
        "        i=int(i)\n",
        "        result_list.append(list_st[i])\n",
        "    return result_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wep_2lQrJ5y5"
      },
      "source": [
        "def print_cloud(list_token):\n",
        "    dic_list=[]\n",
        "    for sentence in list_token:\n",
        "        for x in sentence:\n",
        "            dic_list.append(x)\n",
        "    count=Counter(dic_list)\n",
        "    words = dict(count.most_common())\n",
        "\n",
        "    %matplotlib inline \n",
        "    import matplotlib \n",
        "    from IPython.display import set_matplotlib_formats \n",
        "\n",
        "    matplotlib.rc('font',family = 'Malgun Gothic') \n",
        "\n",
        "    set_matplotlib_formats('retina') \n",
        "\n",
        "    matplotlib.rc('axes',unicode_minus = False)\n",
        "\n",
        "    from wordcloud import WordCloud\n",
        "    wc = WordCloud(max_font_size=200, background_color='white', width=1600, height=1000, font_path = dir_font)\n",
        "    wc.generate(' '.join(words))\n",
        "    plt.figure(figsize=(20,10), facecolor='k')\n",
        "    plt.imshow(wc)\n",
        "    plt.axis('off')\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SibQvGxM2k"
      },
      "source": [
        "def print_100(text):\n",
        "    start = 0\n",
        "    end = 100\n",
        "\n",
        "    while True:\n",
        "        print(text[start:end],end='\\n')\n",
        "\n",
        "        if len(text[end:]) < 101:\n",
        "            print(text[end:])\n",
        "            break\n",
        "        else:\n",
        "            start = end + 1\n",
        "            end += 101"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqTuzRU8e4au"
      },
      "source": [
        "# 데이터처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0iKviJ3e3s_"
      },
      "source": [
        "con=pd.read_csv('saha.csv') # 처리할 데이터 입력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TERN78rb0Q4Q"
      },
      "source": [
        "con_real=pd.read_csv('saha.csv') # 처리할 데이터 입력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGcVjw0k26t7"
      },
      "source": [
        "con_real['text'][166]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZYKds4GAyjn"
      },
      "source": [
        "con['token_text']=con['text'].swifter.apply(lambda x:tokenize(x))\n",
        "con['token_title']=con['title'].swifter.apply(lambda x:tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmq-wuoFypCK"
      },
      "source": [
        "#쓰레기 데이터 삭제\n",
        "\n",
        "con=del_sentence(['롯데'],con)\n",
        "con.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efz24m8Tzgvp"
      },
      "source": [
        "con=con.drop(['index','Unnamed: 0'],axis=1)\n",
        "con"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQqVGmIhfeFf"
      },
      "source": [
        "#데이터 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4VA2ES0G-zB"
      },
      "source": [
        "tokenized_doc=con['token_text']\n",
        "tokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8wtNX6mfsVr"
      },
      "source": [
        "token_title=con['token_title']\n",
        "token_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxr8RB74HAYw"
      },
      "source": [
        "con['text'][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRxSy-sfpjR"
      },
      "source": [
        "# 데이터 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGGWHYjK5NPl"
      },
      "source": [
        "#분석할 데이터 입력\n",
        "analy_data=tokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woDa4gi0iypu"
      },
      "source": [
        "\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(analy_data)\n",
        "corpus = [dictionary.doc2bow(text) for text in analy_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hDQCve0iynT"
      },
      "source": [
        "import gensim\n",
        "NUM_TOPICS = 7 #20개의 토픽, k=20\n",
        "num_words=7\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "topics = ldamodel.print_topics(num_words=num_words)\n",
        "list_st=[]\n",
        "\n",
        "for topic in topics:\n",
        "    store_list=[]\n",
        "    store=topic[1].split('*')\n",
        "    for i in range(1,1+num_words):\n",
        "        k=store[i].split('+')[0][1:-1]\n",
        "        if k[-1]=='\"':\n",
        "            store_list.append(k[0:-1])\n",
        "        else:\n",
        "            store_list.append(k)\n",
        "\n",
        "    list_st.append(store_list)\n",
        "\n",
        "\n",
        "for i in range(len(list_st)):\n",
        "    print(\"topic%d: \"%(i),end='')\n",
        "    print(list_st[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpwqovZ3iylG"
      },
      "source": [
        " for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "    if i==5:\n",
        "        break\n",
        "    print(i,'번째 문서의 topic 비율은',topic_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbQi_00qiygE"
      },
      "source": [
        "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
        "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
        "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
        "topictable['text']=con['text']\n",
        "topictable['토픽 내용']=find_topic(topictable['가장 비중이 높은 토픽'],list_st)\n",
        "topictable[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2YecdT6khXg"
      },
      "source": [
        "most_topic_count=topictable['가장 비중이 높은 토픽'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlxk9_kkkbih"
      },
      "source": [
        "most_topic_count.sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcET2X_3Oqjo"
      },
      "source": [
        "fr_dict={}\n",
        "for i in range(len(most_topic_count)):\n",
        "    fr_dict[int(most_topic_count.sort_index().index[i])]=most_topic_count.sort_index().values[i]\n",
        "fr_dict\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLUzDKyXTBFQ"
      },
      "source": [
        "plt.bar(fr_dict.keys(),fr_dict.values())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw3rxO3Gg0J-"
      },
      "source": [
        "print_topic_num=3  #출력할 토픽의 수\n",
        "print_problem_num=5 # 출력할 청원 수\n",
        "num=0\n",
        "for i in most_topic_count.index:\n",
        "    if num>=print_topic_num:\n",
        "        break\n",
        "    print(str(i)+'번째 토픽: ',end='')\n",
        "    for k in list_st[int(i)]:\n",
        "        print(k+', ',end='')\n",
        "    print('\\n')\n",
        "\n",
        "    num_1=0\n",
        "    for k in topictable.values:\n",
        "        if num_1>=print_problem_num:\n",
        "            break\n",
        "        \n",
        "        if k[1]==i:\n",
        "            print_100(k[4])\n",
        "            print('\\n')\n",
        "            num_1+=1\n",
        "    num+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA76MFSDg0HA"
      },
      "source": [
        "for k in topictable.values:\n",
        "    print(k[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8dCrhfg0Sm"
      },
      "source": [
        "#word cloud 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AGAHciODfFY"
      },
      "source": [
        "words=print_cloud(analy_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7jt9lcKiyUq"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EwlKJCUBIk_"
      },
      "source": [
        "num=0\n",
        "for key,value in words.items():\n",
        "    print('%s : %d'%(key,value))\n",
        "    num+=1\n",
        "    if num==20:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZ9YQtb4f-k"
      },
      "source": [
        "words.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XE_9FlhB49w"
      },
      "source": [
        "df=pd.DataFrame({\"word\":words.keys(), \"count\":words.values()}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rn7R4N15p8O"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGZoBonX4mQ2"
      },
      "source": [
        "df.to_csv('word_count_saha.csv',encoding='CP949')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clSfmQKlCbRV"
      },
      "source": [
        "def find_text(word):\n",
        "    num_list=[]\n",
        "    text_list=[]\n",
        "    word_list=[]\n",
        "    title_list=[]\n",
        "    for i in range(len(analy_data)):\n",
        "        if word in analy_data[i]:\n",
        "            print('%d번째 글: '%i,end='')\n",
        "            print_100(con['text'][i])\n",
        "            print('\\n\\n')\n",
        "            word_list.append(word)\n",
        "            text_list.append(con['text'][i])\n",
        "            num_list.append(i)\n",
        "            title_list.append(con['title'][i])\n",
        "    df=pd.DataFrame({\"word\":word_list, \"title\":title_list, \"text\":text_list}) \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH_hUPJkgM3D"
      },
      "source": [
        "# 토픽과 단어에 대한 텍스트 찾기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jRcSUdBaPlz"
      },
      "source": [
        "df=find_text('재개발')\n",
        "df['text']=df['text'].swifter.apply(lambda x:clean_sentence(x))\n",
        "df.to_csv('재개발_사하구.csv',encoding='CP949')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs7MP0nJ8Gwd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYNeDTic8O5j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drQqkhMJ8e2R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}